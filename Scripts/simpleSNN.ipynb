{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/enlupi/SNN-MUC/blob/main/Scripts/DTFastSim.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rcbi-PGdeIDh"
   },
   "source": [
    "# Data Simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GenEvents as GE\n",
    "import PlotEvents as PE\n",
    "import EventData as ED\n",
    "from Params import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "VLIjTLXxqimK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm \n",
    "\n",
    "import snntorch as snn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import snntorch.spikeplot as splt\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from snntorch import surrogate\n",
    "\n",
    "from snntorch import spikegen\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datasets - preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:49<00:00, 603.53it/s]\n",
      "100%|██████████| 5000/5000 [00:09<00:00, 535.80it/s]\n"
     ]
    }
   ],
   "source": [
    "num_events = 30000\n",
    "num_events_test = 5000\n",
    "\n",
    "events_arr, muon_list, max_n = GE.generate_noisy_evts(num_events, noise_frac=1, bkg_frac=0.5)\n",
    "events_arr_test, muon_list_test, max_n_test = GE.generate_noisy_evts(num_events_test, noise_frac=1, bkg_frac=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_target(muon_list):\n",
    "    target = np.zeros(shape=(len(muon_list), NLAYERS, NWIRES), dtype=np.int16)\n",
    "    for i, evt in tqdm.tqdm(enumerate(muon_list)):\n",
    "        for hit in evt:\n",
    "            layer, wire = hit['layer']-1, hit['wire_num']-1\n",
    "\n",
    "            if hit['signal'] == True:\n",
    "                target[i, layer, wire] = 1\n",
    "\n",
    "    return torch.tensor(target, dtype=torch.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30000it [00:00, 71569.25it/s]\n",
      "5000it [00:00, 81952.97it/s]\n"
     ]
    }
   ],
   "source": [
    "target = gen_target(muon_list)\n",
    "target_test = gen_target(muon_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'layer': 3,\n",
       "  'wire_num': 2,\n",
       "  'bx': 516,\n",
       "  'tdc': 27,\n",
       "  'label': 0,\n",
       "  't0': 500.3666666666667,\n",
       "  'psi': 0.3154152764339655,\n",
       "  'x0': 4.176478920061371,\n",
       "  'signal': False},\n",
       " {'layer': 3,\n",
       "  'wire_num': 2,\n",
       "  'bx': 504,\n",
       "  'tdc': 27,\n",
       "  'label': 1,\n",
       "  't0': 500.3666666666667,\n",
       "  'psi': 0.3154152764339655,\n",
       "  'x0': 4.176478920061371,\n",
       "  'signal': True},\n",
       " {'layer': 3,\n",
       "  'wire_num': 3,\n",
       "  'bx': 493,\n",
       "  'tdc': 21,\n",
       "  'label': 0,\n",
       "  't0': 500.3666666666667,\n",
       "  'psi': 0.3154152764339655,\n",
       "  'x0': 4.176478920061371,\n",
       "  'signal': False},\n",
       " {'layer': 2,\n",
       "  'wire_num': 3,\n",
       "  'bx': 514,\n",
       "  'tdc': 9,\n",
       "  'label': -1,\n",
       "  't0': 500.3666666666667,\n",
       "  'psi': 0.3154152764339655,\n",
       "  'x0': 4.176478920061371,\n",
       "  'signal': True},\n",
       " {'layer': 1,\n",
       "  'wire_num': 2,\n",
       "  'bx': 502,\n",
       "  'tdc': 0,\n",
       "  'label': -1,\n",
       "  't0': 500.3666666666667,\n",
       "  'psi': 0.3154152764339655,\n",
       "  'x0': 4.176478920061371,\n",
       "  'signal': True},\n",
       " {'layer': 2,\n",
       "  'wire_num': 1,\n",
       "  'bx': 517,\n",
       "  'tdc': 13,\n",
       "  'label': 0,\n",
       "  't0': 500.3666666666667,\n",
       "  'psi': 0.3154152764339655,\n",
       "  'x0': 4.176478920061371,\n",
       "  'signal': False},\n",
       " {'layer': 4,\n",
       "  'wire_num': 3,\n",
       "  'bx': 507,\n",
       "  'tdc': 24,\n",
       "  'label': -1,\n",
       "  't0': 500.3666666666667,\n",
       "  'psi': 0.3154152764339655,\n",
       "  'x0': 4.176478920061371,\n",
       "  'signal': True}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "muon_list[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, input_data, target, transform=None):\n",
    "        self.input_data = input_data\n",
    "        self.target = target\n",
    "        \n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        input_sample = self.input_data[idx]\n",
    "        target_sample = self.target[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            input_sample = self.transform(input_sample)\n",
    "            \n",
    "        return input_sample, target_sample\n",
    "    \n",
    "def pad(muon_hits, max_n_hit):\n",
    "    padded_array = np.zeros(max_n_hit, dtype=ED.hit_dtype)\n",
    "    for i, hit in enumerate(muon_hits):\n",
    "        padded_array[i]['bx']     = hit['bx']\n",
    "        padded_array[i]['tdc']    = hit['tdc']\n",
    "        padded_array[i]['label']  = hit['label']\n",
    "        padded_array[i]['signal'] = hit['signal']\n",
    "\n",
    "    return padded_array\n",
    "\n",
    "def convert_to(muon_hits, size, target_dtype=np.int16,\n",
    "               features=['layer', 'wire_num', 'bx', 'tdc', 'signal']):\n",
    "    padded_array = np.zeros(shape=(size, len(features)), dtype=target_dtype)\n",
    "    for i, hit in enumerate(muon_hits):\n",
    "        for j, f in enumerate(features):\n",
    "            padded_array[i,j] = hit[f]\n",
    "\n",
    "    return padded_array\n",
    "\n",
    "\n",
    "\n",
    "def to_tensor_and_dtype(input_variable, target_dtype=torch.float32):\n",
    "    \n",
    "    # Convert to PyTorch tensor\n",
    "    tensor = torch.tensor(input_variable)\n",
    "    # Force the tensor to have the specified dtype\n",
    "    tensor = tensor.to(target_dtype)\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "# Define a transform for 4x4 matrices\n",
    "pad_transform = transforms.Compose([\n",
    "    lambda x: convert_to(x, size=max_n, target_dtype=np.int16,\n",
    "                         features=['layer', 'wire_num', 'bx', 'tdc', 'signal'])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(muon_list, target, transform=pad_transform)\n",
    "test_dataset = CustomDataset(muon_list_test, target_test, transform=pad_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 60\n",
    "nw=0\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader._SingleProcessDataLoaderIter'>\n"
     ]
    }
   ],
   "source": [
    "data = iter(train_loader)\n",
    "data_it, targets_it = next(data)\n",
    "\n",
    "print(type(data))\n",
    "\n",
    "# Spiking Data\n",
    "#spike_data = spikegen.rate(data_it, num_steps=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_to_matrix(event,rec=True):\n",
    "    tmp_matrix = event['tdc']*(25/30)+event['bx']*25\n",
    "    tmp_matrix[tmp_matrix <0] = 0\n",
    "    \n",
    "    min_value = np.min(tmp_matrix)\n",
    "    max_value = np.max(tmp_matrix)\n",
    "    \n",
    "    tmp_matrix = (tmp_matrix - min_value) / (max_value - min_value)\n",
    "    \n",
    "    if rec:\n",
    "        tmp_matrix[tmp_matrix > 0] = 1/tmp_matrix[tmp_matrix > 0]\n",
    "        \n",
    "        min_value = np.min(tmp_matrix)\n",
    "        max_value = np.max(tmp_matrix)\n",
    "\n",
    "        tmp_matrix = (tmp_matrix - min_value) / (max_value - min_value)\n",
    "        \n",
    "    return tmp_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_batch_accuracy(net, data, targets, batch_size, train=False):\n",
    "    output, _ = net(data.view(batch_size, -1))\n",
    "    _, idx = output.sum(dim=0).max(1)\n",
    "    acc = np.mean((targets == idx).detach().cpu().numpy())\n",
    "\n",
    "    if train:\n",
    "        print(f\"Train set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "    else:\n",
    "        print(f\"Test set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "\n",
    "def train_printer(net, data, targets, batch_size, epoch,counter, iter_counter,\n",
    "                  loss_hist, test_loss_hist, test_data, test_targets):\n",
    "    print(f\"Epoch {epoch}, Iteration {iter_counter}\")\n",
    "    print(f\"Train Set Loss: {loss_hist[counter]:.2f}\")\n",
    "    print(f\"Test Set Loss: {test_loss_hist[counter]:.2f}\")\n",
    "    print_batch_accuracy(net, data, targets, batch_size, train=True)\n",
    "    print_batch_accuracy(net, test_data, test_targets, batch_size, train=False)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train set\n",
    "input_data_r=np.zeros((num_events,NLAYERS,NWIRES))\n",
    "input_data_t=np.zeros((num_events,NLAYERS,NWIRES))\n",
    "\n",
    "for i in range(num_events):\n",
    "    input_data_r[i]=event_to_matrix(events_arr['mc'][i],True)\n",
    "    input_data_t[i]=event_to_matrix(events_arr['mc'][i],False)\n",
    "    \n",
    "target_data = np.ones(num_events)\n",
    "for i in range(num_events):\n",
    "    if events_arr[i]['n_true_hits'] == 0:\n",
    "        target_data[i] = 0\n",
    "        \n",
    "#test set\n",
    "test_data_r=np.zeros((num_events_test,NLAYERS,NWIRES))\n",
    "test_data_t=np.zeros((num_events_test,NLAYERS,NWIRES))\n",
    "\n",
    "for i in range(num_events_test):\n",
    "    test_data_r[i]=event_to_matrix(events_arr_test['mc'][i],True)\n",
    "    test_data_t[i]=event_to_matrix(events_arr['mc'][i],False)\n",
    "    \n",
    "target_test = np.ones(num_events_test)\n",
    "for i in range(num_events_test):\n",
    "    if events_arr_test[i]['n_true_hits'] == 0:\n",
    "        target_test[i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Architecture\n",
    "num_inputs = NLAYERS*NWIRES\n",
    "num_hidden = 100\n",
    "num_outputs = 2\n",
    "\n",
    "# Temporal Dynamics\n",
    "num_steps = 25\n",
    "beta = 0.7\n",
    "\n",
    "batch_size = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"Simple spiking neural network in snntorch.\"\"\"\n",
    "\n",
    "    def __init__(self, input_feat, hidden,out_feat,timesteps):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_feat = input_feat # number of input neurons \n",
    "        self.hidden = hidden # number of hidden neurons\n",
    "        self.out_feat = out_feat # number of output neurons\n",
    "        \n",
    "        self.timesteps = timesteps # number of time steps to simulate the network\n",
    "        spike_grad = surrogate.fast_sigmoid() # surrogate gradient function\n",
    "        \n",
    "        self.fc_in = nn.Linear(in_features=self.input_feat, out_features=self.hidden)\n",
    "        self.lif_in = snn.Leaky(beta=beta,spike_grad=spike_grad)\n",
    "        \n",
    "        self.fc_out = nn.Linear(in_features=self.hidden, out_features=self.out_feat)\n",
    "        self.lif_out = snn.Leaky(beta=beta,spike_grad=spike_grad)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass for several time steps.\"\"\"\n",
    "\n",
    "        # Initalize membrane potential\n",
    "        mem1 = self.lif_in.init_leaky()\n",
    "        mem2 = self.lif_out.init_leaky()\n",
    "\n",
    "        # Record the final layer\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "\n",
    "        # Loop over \n",
    "        for step in range(self.timesteps):\n",
    "                \n",
    "            cur1 = self.fc_in(x)\n",
    "            spk1, mem1 = self.lif_in(cur1, mem1)\n",
    "            cur2 = self.fc_out(spk1)\n",
    "            spk2, mem2 = self.lif_out(cur2, mem2)\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "\n",
    "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)\n",
    "    \n",
    "net = Net(NLAYERS*NWIRES,20,2,25).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SNN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "loss_hist = []\n",
    "test_loss_hist = []\n",
    "counter = 0\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "\n",
    "train_loader_rate = DataLoader(train_rate, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader_rate = DataLoader(test_rate, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# Outer training loop\n",
    "for epoch in range(num_epochs):\n",
    "    iter_counter = 0\n",
    "    train_batch = iter(train_loader_rate)\n",
    "\n",
    "    # Minibatch training loop\n",
    "    for data, targets in train_batch:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        net.train()\n",
    "        spk_rec, mem_rec = net(data.view(batch_size, -1))\n",
    "\n",
    "        # initialize the loss & sum over time\n",
    "        loss_val = torch.zeros((1), dtype=torch.float, device=device)\n",
    "        for step in range(num_steps):\n",
    "            loss_val += loss(mem_rec[step], targets)\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "        # Test set\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            test_data, test_targets = next(iter(test_loader_rate))\n",
    "            test_data = test_data.to(device)\n",
    "            test_targets = test_targets.to(device)\n",
    "\n",
    "            # Test set forward pass\n",
    "            test_spk, test_mem = net(test_data.view(batch_size, -1))\n",
    "\n",
    "            # Test set loss\n",
    "            test_loss = torch.zeros((1), dtype=torch.float, device=device)\n",
    "            for step in range(num_steps):\n",
    "                test_loss += loss(test_mem[step], test_targets)\n",
    "            test_loss_hist.append(test_loss.item())\n",
    "\n",
    "            # Print train/test loss/accuracy\n",
    "            if counter % 50 == 0:\n",
    "                train_printer(\n",
    "                    net, data, targets, batch_size, epoch,\n",
    "                    counter, iter_counter,\n",
    "                    loss_hist, test_loss_hist,\n",
    "                    test_data, test_targets)\n",
    "            counter += 1\n",
    "            iter_counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss\n",
    "fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "plt.plot(loss_hist)\n",
    "plt.plot(test_loss_hist)\n",
    "plt.title(\"Loss Curves\")\n",
    "plt.legend([\"Train Loss\", \"Test Loss\"])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "# drop_last switched to False to keep all samples\n",
    "test_loader = DataLoader(test_rate, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    \n",
    "    for data, targets in test_loader:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "    \n",
    "        # forward pass\n",
    "        test_spk, _ = net(data.view(data.size(0), -1))\n",
    "\n",
    "        # calculate total accuracy\n",
    "        _, predicted = test_spk.sum(dim=0).max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "print(f\"Total correctly classified test set images: {correct}/{total}\")\n",
    "print(f\"Test Set Accuracy: {100 * correct / total:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
