{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffc96586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import GenEvents as ge\n",
    "import PlotEvents as pe\n",
    "import EventData as ed\n",
    "import Params as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "742a0b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm \n",
    "import math\n",
    "\n",
    "import snntorch as snn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import snntorch.spikeplot as splt\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from snntorch import surrogate\n",
    "import snntorch.functional as SF\n",
    "\n",
    "from snntorch import spikegen\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b09128b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30000/30000 [00:29<00:00, 1028.03it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [00:07<00:00, 1122.83it/s]\n"
     ]
    }
   ],
   "source": [
    "num_events = 30000\n",
    "num_events_test = 8000\n",
    "\n",
    "events_arr, muon_list, max_n = ge.generate_noisy_evts(num_events, noise_frac=0.4, bkg_frac=0.5)\n",
    "events_arr_test, muon_list_test, max_n_test = ge.generate_noisy_evts(num_events_test, noise_frac=0.4, bkg_frac=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9ba5a549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_train(muon_list):\n",
    "    trueTarget = []\n",
    "    for event in muon_list:\n",
    "        trueEvent = []\n",
    "        for hit in event:\n",
    "            if hit[\"signal\"] == True:\n",
    "                trueEvent.append(hit)\n",
    "    \n",
    "        trueTarget.append(trueEvent)   \n",
    "        \n",
    "    return trueTarget\n",
    "\n",
    "target = get_target_train(muon_list)\n",
    "target_test = get_target_train(muon_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "99b495ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, input_data, target, transform=None):\n",
    "        self.input_data = input_data\n",
    "        self.target = target\n",
    "        \n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        input_sample = self.input_data[idx]\n",
    "        target_sample = self.target[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            input_sample = self.transform(input_sample)\n",
    "            target_sample = torch.tensor(target_sample,dtype=torch.float32)\n",
    "            \n",
    "        return input_sample, target_sample\n",
    "\n",
    "\n",
    "# Transformations\n",
    "\n",
    "# transform each event (muon_hits list) to np.array of fixed size\n",
    "def convert_to(muon_hits, size, target_dtype=np.int16,\n",
    "               features=['layer', 'wire_num', 'bx', 't0']):\n",
    "    padded_array = np.zeros(shape=(size, len(features)), dtype=target_dtype)\n",
    "    for i, hit in enumerate(muon_hits):\n",
    "        for j, f in enumerate(features):\n",
    "            padded_array[i,j] = hit[f]  # BEWARE: implicit type conversions going on here\n",
    "\n",
    "    return padded_array\n",
    "\n",
    "# converts to Torch tensor of desired type\n",
    "def to_tensor_and_dtype(input_variable, target_dtype=torch.float32):\n",
    "    \n",
    "    # Convert to PyTorch tensor\n",
    "    tensor = torch.tensor(input_variable)\n",
    "    # Force the tensor to have the specified dtype\n",
    "    tensor = tensor.to(target_dtype)\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "feature_list = ['layer', 'wire_num', 'bx', 't0']\n",
    "transform = transforms.Compose([\n",
    "    lambda x: convert_to(x, size=max_n, target_dtype=np.float32,\n",
    "                         features=feature_list),\n",
    "    lambda x: to_tensor_and_dtype(x, target_dtype=torch.float32)\n",
    "])\n",
    "\n",
    "train_dataset = CustomDataset(muon_list, target, transform=transform)\n",
    "test_dataset = CustomDataset(muon_list_test, target_test, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6bdf6f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_spikegen(data_it, num_step=40):\n",
    "    spike_data = torch.zeros(size=(num_step,batch_size,ps.NLAYERS,ps.NWIRES), dtype=data_it.dtype)\n",
    "    for n_batch, evt in enumerate(data_it):\n",
    "        for hit in evt:\n",
    "            layer = int(hit[0])\n",
    "            wire = int(hit[1])\n",
    "            bx = int(hit[2])\n",
    "            t0 = math.floor(hit[3])\n",
    "            if bx != 0:\n",
    "                spike_data[bx-t0+ps.bx_oot, n_batch, layer-1, wire-1] = 1\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "    return spike_data  \n",
    "\n",
    "def get_target_train(target,num_step=40,plain=True):\n",
    "\n",
    "    targets=[]\n",
    "    for event in target:\n",
    "        \n",
    "        if plain:\n",
    "            spike_data = np.zeros(num_step)\n",
    "        else:\n",
    "            spike_data=np.zeros((num_step,ps.NLAYERS,ps.NWIRES))\n",
    "        \n",
    "        if event:\n",
    "            \n",
    "            startTime = min(500,min([hit[\"bx\"] for hit in event]))\n",
    "                        \n",
    "            for hit in event:\n",
    "                bx = hit[\"bx\"]\n",
    "                time = bx-startTime\n",
    "                if plain:\n",
    "                    spike_data[time] = 1\n",
    "                \n",
    "                else:\n",
    "\n",
    "                    layer = hit[\"layer\"]\n",
    "                    wire = hit[\"wire_num\"]\n",
    "        \n",
    "                    spike_data[time, layer-1, wire-1] = 1\n",
    "        \n",
    "        targets.append(spike_data)\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "21c3b358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Architecture\n",
    "num_inputs = ps.NLAYERS*ps.NWIRES\n",
    "num_hidden = 100\n",
    "num_outputs = 1#num_inputs\n",
    "\n",
    "# Temporal Dynamics\n",
    "num_steps = 40\n",
    "beta = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5e914c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"Simple spiking neural network in snntorch.\"\"\"\n",
    "\n",
    "    def __init__(self, input_feat, hidden,out_feat,timesteps):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_feat = input_feat # number of input neurons \n",
    "        self.hidden = hidden # number of hidden neurons\n",
    "        self.out_feat = out_feat # number of output neurons\n",
    "        \n",
    "        self.timesteps = timesteps # number of time steps to simulate the network\n",
    "        spike_grad = surrogate.fast_sigmoid() # surrogate gradient function\n",
    "        \n",
    "        self.fc_in = nn.Linear(in_features=self.input_feat, out_features=self.hidden)\n",
    "        self.lif_in = snn.Leaky(beta=beta,spike_grad=spike_grad,threshold=0.9)\n",
    "        \n",
    "        self.fc_out = nn.Linear(in_features=self.hidden, out_features=self.out_feat)\n",
    "        self.lif_out = snn.Leaky(beta=beta,spike_grad=spike_grad,threshold=0.9)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass for several time steps.\"\"\"\n",
    "\n",
    "        # Initalize membrane potential\n",
    "        mem1 = self.lif_in.init_leaky()\n",
    "        mem2 = self.lif_out.init_leaky()\n",
    "\n",
    "        # Record the final layer\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "\n",
    "        # Loop over \n",
    "        for step in range(self.timesteps):\n",
    "                \n",
    "            cur1 = self.fc_in(x[step])\n",
    "            spk1, mem1 = self.lif_in(cur1, mem1)\n",
    "            cur2 = self.fc_out(spk1)\n",
    "            spk2, mem2 = self.lif_out(cur2, mem2)\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "\n",
    "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)\n",
    "    \n",
    "net = Net(num_inputs, num_hidden, num_outputs, num_steps).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "899b47f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_batch_accuracy(net, data, targets, batch_size, train=False):\n",
    "    output, _ = net(data)\n",
    "    _, idx = output.sum(dim=0).max(1)\n",
    "    acc = np.mean((targets == idx).detach().cpu().numpy())\n",
    "    \n",
    "    if train:\n",
    "        print(f\"Train set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "    else:\n",
    "        print(f\"Test set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "\n",
    "def train_printer(net, data, targets, batch_size, epoch, counter, iter_counter,\n",
    "                  loss_hist, test_loss_hist, test_data, test_targets):\n",
    "    print(f\"Epoch {epoch}, Iteration {iter_counter}\")\n",
    "    print(f\"Train Set Loss: {loss_hist[counter]:.2f}\")\n",
    "    print(f\"Test Set Loss: {test_loss_hist[counter]:.2f}\")\n",
    "    print_batch_accuracy(net, data, targets, batch_size, train=True)\n",
    "    print_batch_accuracy(net, test_data, test_targets, batch_size, train=False)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "613d80c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 60\n",
    "nw=0\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f9d74a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Batch: 0\n",
      "Iteration: 0\n",
      "Loss: 2.6000003814697266 \n",
      "\n",
      "Epoch: 0\n",
      "Batch: 50\n",
      "Iteration: 50\n",
      "Loss: 2.9512503147125244 \n",
      "\n",
      "Epoch: 0\n",
      "Batch: 100\n",
      "Iteration: 100\n",
      "Loss: 2.7400004863739014 \n",
      "\n",
      "Epoch: 0\n",
      "Batch: 150\n",
      "Iteration: 150\n",
      "Loss: 3.2400004863739014 \n",
      "\n",
      "Epoch: 0\n",
      "Batch: 200\n",
      "Iteration: 200\n",
      "Loss: 3.9225010871887207 \n",
      "\n",
      "Epoch: 0\n",
      "Batch: 250\n",
      "Iteration: 250\n",
      "Loss: 3.9700005054473877 \n",
      "\n",
      "Epoch: 0\n",
      "Batch: 300\n",
      "Iteration: 300\n",
      "Loss: 4.2962493896484375 \n",
      "\n",
      "Epoch: 0\n",
      "Batch: 350\n",
      "Iteration: 350\n",
      "Loss: 3.923750400543213 \n",
      "\n",
      "Epoch: 0\n",
      "Batch: 400\n",
      "Iteration: 400\n",
      "Loss: 3.081249713897705 \n",
      "\n",
      "Epoch: 0\n",
      "Batch: 450\n",
      "Iteration: 450\n",
      "Loss: 2.9337499141693115 \n",
      "\n",
      "Epoch: 1\n",
      "Batch: 0\n",
      "Iteration: 500\n",
      "Loss: 3.970001220703125 \n",
      "\n",
      "Epoch: 1\n",
      "Batch: 50\n",
      "Iteration: 550\n",
      "Loss: 3.2637503147125244 \n",
      "\n",
      "Epoch: 1\n",
      "Batch: 100\n",
      "Iteration: 600\n",
      "Loss: 3.3187499046325684 \n",
      "\n",
      "Epoch: 1\n",
      "Batch: 150\n",
      "Iteration: 650\n",
      "Loss: 4.159999847412109 \n",
      "\n",
      "Epoch: 1\n",
      "Batch: 200\n",
      "Iteration: 700\n",
      "Loss: 2.4150004386901855 \n",
      "\n",
      "Epoch: 1\n",
      "Batch: 250\n",
      "Iteration: 750\n",
      "Loss: 3.976250171661377 \n",
      "\n",
      "Epoch: 1\n",
      "Batch: 300\n",
      "Iteration: 800\n",
      "Loss: 3.5737497806549072 \n",
      "\n",
      "Epoch: 1\n",
      "Batch: 350\n",
      "Iteration: 850\n",
      "Loss: 4.208749771118164 \n",
      "\n",
      "Epoch: 1\n",
      "Batch: 400\n",
      "Iteration: 900\n",
      "Loss: 2.859999656677246 \n",
      "\n",
      "Epoch: 1\n",
      "Batch: 450\n",
      "Iteration: 950\n",
      "Loss: 3.8500001430511475 \n",
      "\n",
      "Epoch: 2\n",
      "Batch: 0\n",
      "Iteration: 1000\n",
      "Loss: 3.277500629425049 \n",
      "\n",
      "Epoch: 2\n",
      "Batch: 50\n",
      "Iteration: 1050\n",
      "Loss: 2.711249589920044 \n",
      "\n",
      "Epoch: 2\n",
      "Batch: 100\n",
      "Iteration: 1100\n",
      "Loss: 2.897500514984131 \n",
      "\n",
      "Epoch: 2\n",
      "Batch: 150\n",
      "Iteration: 1150\n",
      "Loss: 3.0137500762939453 \n",
      "\n",
      "Epoch: 2\n",
      "Batch: 200\n",
      "Iteration: 1200\n",
      "Loss: 3.7325010299682617 \n",
      "\n",
      "Epoch: 2\n",
      "Batch: 250\n",
      "Iteration: 1250\n",
      "Loss: 3.0712502002716064 \n",
      "\n",
      "Epoch: 2\n",
      "Batch: 300\n",
      "Iteration: 1300\n",
      "Loss: 4.2275004386901855 \n",
      "\n",
      "Epoch: 2\n",
      "Batch: 350\n",
      "Iteration: 1350\n",
      "Loss: 3.8700006008148193 \n",
      "\n",
      "Epoch: 2\n",
      "Batch: 400\n",
      "Iteration: 1400\n",
      "Loss: 2.7987499237060547 \n",
      "\n",
      "Epoch: 2\n",
      "Batch: 450\n",
      "Iteration: 1450\n",
      "Loss: 3.869999885559082 \n",
      "\n",
      "Epoch: 3\n",
      "Batch: 0\n",
      "Iteration: 1500\n",
      "Loss: 3.601249933242798 \n",
      "\n",
      "Epoch: 3\n",
      "Batch: 50\n",
      "Iteration: 1550\n",
      "Loss: 3.652500629425049 \n",
      "\n",
      "Epoch: 3\n",
      "Batch: 100\n",
      "Iteration: 1600\n",
      "Loss: 2.9262499809265137 \n",
      "\n",
      "Epoch: 3\n",
      "Batch: 150\n",
      "Iteration: 1650\n",
      "Loss: 2.8249998092651367 \n",
      "\n",
      "Epoch: 3\n",
      "Batch: 200\n",
      "Iteration: 1700\n",
      "Loss: 3.59125018119812 \n",
      "\n",
      "Epoch: 3\n",
      "Batch: 250\n",
      "Iteration: 1750\n",
      "Loss: 3.1287500858306885 \n",
      "\n",
      "Epoch: 3\n",
      "Batch: 300\n",
      "Iteration: 1800\n",
      "Loss: 2.8137495517730713 \n",
      "\n",
      "Epoch: 3\n",
      "Batch: 350\n",
      "Iteration: 1850\n",
      "Loss: 2.260000467300415 \n",
      "\n",
      "Epoch: 3\n",
      "Batch: 400\n",
      "Iteration: 1900\n",
      "Loss: 3.0687501430511475 \n",
      "\n",
      "Epoch: 3\n",
      "Batch: 450\n",
      "Iteration: 1950\n",
      "Loss: 3.5350003242492676 \n",
      "\n",
      "Epoch: 4\n",
      "Batch: 0\n",
      "Iteration: 2000\n",
      "Loss: 3.661250114440918 \n",
      "\n",
      "Epoch: 4\n",
      "Batch: 50\n",
      "Iteration: 2050\n",
      "Loss: 3.7262516021728516 \n",
      "\n",
      "Epoch: 4\n",
      "Batch: 100\n",
      "Iteration: 2100\n",
      "Loss: 4.109999179840088 \n",
      "\n",
      "Epoch: 4\n",
      "Batch: 150\n",
      "Iteration: 2150\n",
      "Loss: 3.9187498092651367 \n",
      "\n",
      "Epoch: 4\n",
      "Batch: 200\n",
      "Iteration: 2200\n",
      "Loss: 2.7850003242492676 \n",
      "\n",
      "Epoch: 4\n",
      "Batch: 250\n",
      "Iteration: 2250\n",
      "Loss: 3.5912506580352783 \n",
      "\n",
      "Epoch: 4\n",
      "Batch: 300\n",
      "Iteration: 2300\n",
      "Loss: 3.3162498474121094 \n",
      "\n",
      "Epoch: 4\n",
      "Batch: 350\n",
      "Iteration: 2350\n",
      "Loss: 2.8899996280670166 \n",
      "\n",
      "Epoch: 4\n",
      "Batch: 400\n",
      "Iteration: 2400\n",
      "Loss: 3.859999418258667 \n",
      "\n",
      "Epoch: 4\n",
      "Batch: 450\n",
      "Iteration: 2450\n",
      "Loss: 3.749999761581421 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "loss_hist = []\n",
    "iter_counter = 0\n",
    "\n",
    "#loss = nn.MSELoss()\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "\n",
    "\n",
    "# Outer training loop\n",
    "for epoch in range(num_epochs):\n",
    "    batch_counter = 0\n",
    "    # Minibatch training loop\n",
    "    for data_it, targets_it in train_loader:\n",
    "        data_it = data_it.to(device)\n",
    "        targets_it = targets_it.to(device)\n",
    "\n",
    "        # create spike train\n",
    "        spike_in = custom_spikegen(data_it, num_steps)\n",
    "        spike_in = spike_in.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        net.train()\n",
    "        spk_rec, mem_rec = net(spike_in.view(num_steps, batch_size, -1))\n",
    "\n",
    "        loss_val = torch.tensor(0.)\n",
    "        # initialize the loss & sum over time\n",
    "        for i in range(batch_size):\n",
    "        \n",
    "            loss_val += loss_fn(spk_rec[:,i], targets_it[i,:])\n",
    "        \n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "        \n",
    "        if iter_counter % 50 == 0:\n",
    "            print(\"Epoch:\",epoch)\n",
    "            print(\"Batch:\",batch_counter)\n",
    "            print(\"Iteration:\",iter_counter)\n",
    "            print(\"Loss:\",loss_val.item(),\"\\n\")\n",
    "        \n",
    "        batch_counter += 1\n",
    "        iter_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66136189",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
