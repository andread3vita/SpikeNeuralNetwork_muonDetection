{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93967054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import event simulation files\n",
    "import GenEvents as ge\n",
    "import PlotEvents as pe\n",
    "import EventData as ed\n",
    "import Params as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46c14c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device is cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/anaconda3/envs/torch/lib/python3.8/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449229234/work/c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tqdm \n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import snntorch as snn\n",
    "from snntorch import spikegen\n",
    "import snntorch.spikeplot as splt\n",
    "from snntorch import surrogate\n",
    "import snntorch.functional as SF\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print('Selected device is',device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a56e6d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 100000/100000 [01:26<00:00, 1159.07it/s]\n",
      "100%|█████████████████████████████████████| 5000/5000 [00:04<00:00, 1166.30it/s]\n",
      "100%|███████████████████████████████████| 10000/10000 [00:08<00:00, 1200.53it/s]\n"
     ]
    }
   ],
   "source": [
    "n_evt = 100000\n",
    "n_evt_test = 5000\n",
    "n_evt_val = 10000\n",
    "\n",
    "evt_arr,      muon_list,      max_n      = ge.generate_noisy_evts(n_evt,      noise_frac=0.5, bkg_frac=0.3,ineff=False)\n",
    "evt_arr_test, muon_list_test, max_n_test = ge.generate_noisy_evts(n_evt_test, noise_frac=0.5, bkg_frac=0.3,ineff=False)\n",
    "evt_arr_val,  muon_list_val,  max_n_val  = ge.generate_noisy_evts(n_evt_val,  noise_frac=0.5, bkg_frac=0.3,ineff=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "456a9a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform each event (muon_hits list) to np.array of fixed size\n",
    "def convert_to(muon_list, size, target_dtype, features):\n",
    "    padded_array = np.zeros(shape=(len(muon_list), size, len(features)), dtype=target_dtype)\n",
    "    for i, muon_hits in enumerate(muon_list):\n",
    "        for j, hit in enumerate(muon_hits):\n",
    "            for k, f in enumerate(features):\n",
    "                padded_array[i,j,k] = hit[f]  # BEWARE: implicit type conversions going on here\n",
    "\n",
    "    return torch.tensor(padded_array)\n",
    "\n",
    "feature_list = ['layer', 'wire_num', 'bx', 't0', 'signal']\n",
    "mu_arr      = convert_to(muon_list,      size=max_n,      target_dtype=np.float32, features=feature_list)\n",
    "mu_arr_test = convert_to(muon_list_test, size=max_n_test, target_dtype=np.float32, features=feature_list)\n",
    "mu_arr_val  = convert_to(muon_list_val,  size=max_n_val,  target_dtype=np.float32, features=feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4e24796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_spikegen(data, num_step=40, batch_size=100, linearise=True):\n",
    "\n",
    "    spike_data = torch.zeros(size=(num_step, batch_size, ps.NLAYERS,ps.NWIRES), dtype=data.dtype)\n",
    "    for n_batch, evt in enumerate(data):\n",
    "        for hit in evt:\n",
    "            layer = int(hit[0])\n",
    "            wire = int(hit[1])\n",
    "            bx = int(hit[2])\n",
    "            t0 = math.floor(hit[3])\n",
    "            if bx != 0:\n",
    "                spike_data[bx-t0+ps.bx_oot, n_batch, layer-1, wire-1] = 1\n",
    "            else:\n",
    "                break\n",
    "    if linearise:\n",
    "        spike_data = spike_data.view(num_step, batch_size, -1)\n",
    "                \n",
    "    return spike_data   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f209e54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_target_chamber(muon_arr,num_step=40,numMuons=4):\n",
    "    target = torch.zeros(size=(len(muon_arr),num_step), dtype=muon_arr.dtype)\n",
    "    for i, evt in tqdm.tqdm(enumerate(muon_arr)):\n",
    "        true_hits=[]\n",
    "        \n",
    "        notZeroBx = evt[evt[:,2] > 0][:,2].numpy()\n",
    "        \n",
    "        if len(notZeroBx) == 0:\n",
    "            break\n",
    "        \n",
    "        startTime = min(500,min(notZeroBx))\n",
    "        \n",
    "        for hit in evt:\n",
    "            \n",
    "            if hit[4] == 1.:\n",
    "                true_hits.append(hit[2])\n",
    "        true_hits.sort()\n",
    "        if len(true_hits) < numMuons:\n",
    "            continue\n",
    "        \n",
    "        muon_bx = true_hits[numMuons-1]\n",
    "        \n",
    "        target[i,int(muon_bx-startTime)] = 1\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b08a9099",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, input_data, target, transform=None):\n",
    "        self.data = list(zip(input_data, target))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        sample = self.data[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample\n",
    "\n",
    "\n",
    "# Transformations\n",
    "\n",
    "# converts to Torch tensor of desired type\n",
    "def to_tensor_and_dtype(input_variable, target_dtype=torch.float32):\n",
    "    \n",
    "    # Convert to PyTorch tensor\n",
    "    if type(input_variable) != torch.Tensor:\n",
    "        input_variable = torch.tensor(input_variable)\n",
    "    # Force the tensor to have the specified dtype\n",
    "    tensor = input_variable.to(target_dtype)\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    lambda x: (to_tensor_and_dtype(x[0], target_dtype=torch.float32), x[1])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "791184d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100000it [00:29, 3410.97it/s]\n",
      "5000it [00:01, 3360.72it/s]\n",
      "10000it [00:02, 3414.60it/s]\n"
     ]
    }
   ],
   "source": [
    "target      = gen_target_chamber(mu_arr,num_step=40,numMuons=4)\n",
    "target_test = gen_target_chamber(mu_arr_test,num_step=40,numMuons=4)\n",
    "target_val  = gen_target_chamber(mu_arr_val,num_step=40,numMuons=4)\n",
    "\n",
    "train_dataset = CustomDataset(mu_arr,      target,      transform=transform)\n",
    "test_dataset  = CustomDataset(mu_arr_test, target_test, transform=transform)\n",
    "val_dataset   = CustomDataset(mu_arr_val,  target_val,  transform=transform)\n",
    "\n",
    "batch_size = 100\n",
    "nw=0\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=nw)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, num_workers=nw)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ef4b937",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = ps.NLAYERS*ps.NWIRES\n",
    "num_hidden = 100\n",
    "num_outputs_chamber = 2\n",
    "\n",
    "num_steps = 40\n",
    "window = 16\n",
    "beta = 0.8\n",
    "alpha =0.8\n",
    "threshold=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8f9c6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net1layer2ndOrder(nn.Module):\n",
    "    \"\"\"Simple spiking neural network in snntorch.\"\"\"\n",
    "\n",
    "    def __init__(self, input_feat, hidden,out_feat,window,learnable):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_feat = input_feat # number of input neurons \n",
    "        self.hidden = hidden # number of hidden neurons\n",
    "        self.out_feat = out_feat # number of output neurons\n",
    "        \n",
    "        self.window = window # number of time steps to simulate the network\n",
    "        spike_grad = surrogate.fast_sigmoid() # surrogate gradient function\n",
    "        \n",
    "        self.fc_in = nn.Linear(in_features=self.input_feat, out_features=self.hidden)\n",
    "        self.lif_in = snn.Synaptic(beta=beta,alpha=alpha,threshold=threshold,\n",
    "                                   spike_grad=spike_grad,\n",
    "                                   learn_beta=learnable,learn_threshold=learnable,learn_alpha=learnable)\n",
    "        \n",
    "        self.fc_out = nn.Linear(in_features=self.hidden, out_features=self.out_feat)\n",
    "        self.lif_out = snn.Synaptic(beta=beta,alpha=alpha,threshold=threshold,\n",
    "                                   spike_grad=spike_grad,\n",
    "                                   learn_beta=learnable,learn_threshold=learnable,learn_alpha=learnable)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass for several time steps.\"\"\"\n",
    "        \n",
    "        # Initalize membrane potential\n",
    "        syn1, mem1 = self.lif_in.init_synaptic()\n",
    "        syn2, mem2 = self.lif_out.init_synaptic()\n",
    "        \n",
    "        # Record the final layer\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "\n",
    "        # Loop over \n",
    "        for step in range(self.window):\n",
    "\n",
    "            cur1 = self.fc_in(x[step])\n",
    "            spk1, syn1, mem1 = self.lif_in(cur1, syn1, mem1)\n",
    "            \n",
    "            cur2 = self.fc_out(spk1)\n",
    "            spk2, syn2, mem2 = self.lif_out(cur2, syn2, mem2)\n",
    "            \n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "\n",
    "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e52b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_accuracy(output, targets, type):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    # whole chamber classification\n",
    "    if type == 'chamber':\n",
    "        _, predicted = output.sum(dim=0).max(1)\n",
    "        total = targets.size(0)\n",
    "        correct = (predicted == targets).sum().item()\n",
    "\n",
    "    # cell-by-cell classification\n",
    "    elif type == 'cell':\n",
    "        batch_size = targets.size(0)\n",
    "        _, predicted = output.sum(dim=0).view(batch_size, ps.NLAYERS*ps.NWIRES, -1).max(2)\n",
    "        total = targets.numel()\n",
    "        correct = (predicted == targets).sum().item()\n",
    "\n",
    "    # accuracy on the whole event for cell-by-cell classification\n",
    "    elif type == 'cell_per_evt':\n",
    "        batch_size = targets.size(0)\n",
    "        total = batch_size\n",
    "        _, predicted = output.sum(dim=0).view(batch_size, ps.NLAYERS*ps.NWIRES, -1).max(2)\n",
    "        for i in range(batch_size):\n",
    "            correct += torch.sum((predicted[i] == targets[i]).all())\n",
    "\n",
    "    # accuracy cell-by-cell moment-bymoment for precise timing recontruction\n",
    "    elif type == 'mse_timing':\n",
    "        num_steps = targets.size(0)\n",
    "        batch_size = targets.size(1)\n",
    "        total = targets.numel()\n",
    "        correct = (output == targets).sum().item()\n",
    "\n",
    "    # accuracy on the whole event for timing reconstruction\n",
    "    elif type == 'mse_timing_per_evt':\n",
    "        num_steps = targets.size(0)\n",
    "        batch_size = targets.size(1)\n",
    "        total = batch_size\n",
    "        for i in range(batch_size):\n",
    "            correct += torch.sum((output[:, i] == targets[:, i]).all())\n",
    "\n",
    "    elif type == \"ce_timing\":\n",
    "        num_steps = targets.size(0)\n",
    "        batch_size = targets.size(1)\n",
    "        total = targets.numel()\n",
    "        _, predicted = output.view(num_steps, batch_size, ps.NLAYERS*ps.NWIRES, -1).max(3)\n",
    "        correct = (predicted == targets).sum().item()\n",
    "\n",
    "    elif type == \"bce_timing\":\n",
    "        total = targets.numel()\n",
    "        sigmoid = nn.Sigmoid()\n",
    "        predicted = sigmoid(output)\n",
    "        predicted[predicted <  0.5] = 0\n",
    "        predicted[predicted >= 0.5] = 1\n",
    "        correct = (predicted == targets).sum().item()\n",
    "\n",
    "\n",
    "    return total, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e18d58ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_set(net, data_loader, loss_fn, accuracy_type, batch_size, linearise,num_steps=40):\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        temp_loss = []\n",
    "        for data, targets in data_loader:\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # create spike train\n",
    "            spike_in = custom_spikegen(data, num_steps, batch_size, linearise)\n",
    "            spike_in = spike_in.to(device)\n",
    "            \n",
    "            padding = (0,0,0,0,15, 15)\n",
    "            padded_input = torch.nn.functional.pad(spike_in, padding).to(device)\n",
    "            \n",
    "            loss_val = torch.tensor(0.)\n",
    "            for i in range(num_steps):\n",
    "                window_input = padded_input[i:window+i]\n",
    "                \n",
    "                # forward pass\n",
    "                spk_rec, mem_rec = net(window_input)\n",
    "                output = spk_rec\n",
    "                \n",
    "                # compute loss\n",
    "                loss_val += loss_fn(output, targets[:,i].type(torch.long))\n",
    "                \n",
    "                # calculate total accuracy\n",
    "                tot, corr = comp_accuracy(output, targets[:,i], accuracy_type)\n",
    "                total += tot\n",
    "                correct += corr\n",
    "                \n",
    "            temp_loss.append(loss_val.item())\n",
    "\n",
    "            \n",
    "\n",
    "        mean_loss = np.mean(temp_loss)\n",
    "        acc = correct/total\n",
    "        return mean_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4ca7cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(net, train_loader, val_loader, num_epochs, loss_fn, optimizer, accuracy_type, \n",
    "              batch_size, num_steps = 40,linearise=True):\n",
    "\n",
    "    net.to(device)\n",
    "    \n",
    "    loss_hist = []\n",
    "    loss_val_hist = []\n",
    "    acc_val_hist = []\n",
    "\n",
    "    iter_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        batch_counter = 0\n",
    "        # Minibatch training loop\n",
    "        for data, targets in train_loader:\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # create spike train\n",
    "            spike_in = custom_spikegen(data, num_steps, batch_size, linearise)\n",
    "            spike_in = spike_in.to(device)\n",
    "            \n",
    "            padding = (0,0,0,0,15, 15)\n",
    "            padded_input = nn.functional.pad(spike_in, padding).to(device)\n",
    "            \n",
    "            loss_val = torch.tensor(0.)\n",
    "            for i in range(num_steps):\n",
    "                window_input = padded_input[i:window+i]\n",
    "                \n",
    "                # forward pass\n",
    "                spk_rec, mem_rec = net(window_input)\n",
    "                output = spk_rec\n",
    "                \n",
    "                # compute loss\n",
    "                loss_val += loss_fn(output, targets[:,i].type(torch.long))\n",
    "        \n",
    "            # Gradient calculation + weight update\n",
    "            optimizer.zero_grad()\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Store loss history for future plotting\n",
    "            loss_hist.append(loss_val.item())\n",
    "            if iter_counter % 50 == 0:\n",
    "                print(\"Epoch:\", epoch)\n",
    "                print(\"Batch:\", batch_counter)\n",
    "                print(\"Iteration:\", iter_counter)\n",
    "                print(\"Loss:\", loss_val.item(),\"\\n\")\n",
    "        \n",
    "            batch_counter += 1\n",
    "            iter_counter += 1\n",
    "\n",
    "        # Validation\n",
    "        mean_loss_val, acc_val = accuracy_set(net, val_loader, loss_fn, accuracy_type,batch_size, linearise)\n",
    "\n",
    "        loss_val_hist.append(mean_loss_val)\n",
    "        acc_val_hist.append(acc_val)\n",
    "        print(f\"Validation Set Loss: {mean_loss_val}\")\n",
    "        print(f\"Validation Set Accuracy: {100 *acc_val:.2f}%\")\n",
    "        print(\"\\n--------------------------------------------------\\n\")\n",
    "\n",
    "    return loss_hist, loss_val_hist, acc_val_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57908010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Batch: 0\n",
      "Iteration: 0\n",
      "Loss: 59.376651763916016 \n",
      "\n",
      "Epoch: 0\n",
      "Batch: 50\n",
      "Iteration: 50\n",
      "Loss: 3.565741777420044 \n",
      "\n",
      "Epoch: 0\n",
      "Batch: 100\n",
      "Iteration: 100\n",
      "Loss: 3.013195037841797 \n",
      "\n",
      "Epoch: 0\n",
      "Batch: 150\n",
      "Iteration: 150\n",
      "Loss: 3.114096164703369 \n",
      "\n",
      "Epoch: 0\n",
      "Batch: 200\n",
      "Iteration: 200\n",
      "Loss: 2.5198585987091064 \n",
      "\n",
      "Epoch: 0\n",
      "Batch: 250\n",
      "Iteration: 250\n",
      "Loss: 2.8296031951904297 \n",
      "\n",
      "Epoch: 0\n",
      "Batch: 300\n",
      "Iteration: 300\n",
      "Loss: 3.3197968006134033 \n",
      "\n",
      "Epoch: 0\n",
      "Batch: 350\n",
      "Iteration: 350\n",
      "Loss: 2.7453818321228027 \n",
      "\n",
      "Epoch: 0\n",
      "Batch: 400\n",
      "Iteration: 400\n",
      "Loss: 2.8191676139831543 \n",
      "\n",
      "Epoch: 0\n",
      "Batch: 450\n",
      "Iteration: 450\n",
      "Loss: 2.7915916442871094 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m      7\u001b[0m accuracy_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchamber\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 9\u001b[0m loss_hist, loss_val_hist, acc_val_hist \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43maccuracy_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 40\u001b[0m, in \u001b[0;36mtrain_net\u001b[0;34m(net, train_loader, val_loader, num_epochs, loss_fn, optimizer, accuracy_type, batch_size, num_steps, linearise)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Gradient calculation + weight update\u001b[39;00m\n\u001b[1;32m     39\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 40\u001b[0m \u001b[43mloss_val\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Store loss history for future plotting\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/autograd/function.py:276\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBackwardCFunction\u001b[39;00m(_C\u001b[38;5;241m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n\u001b[1;32m    279\u001b[0m         backward_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cls\u001b[38;5;241m.\u001b[39mbackward  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    280\u001b[0m         vjp_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cls\u001b[38;5;241m.\u001b[39mvjp  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = Net1layer2ndOrder(num_inputs, num_hidden, num_outputs_chamber, window,True).to(device)\n",
    "\n",
    "loss_fn = SF.ce_count_loss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "\n",
    "num_epochs = 5\n",
    "accuracy_type = 'chamber'\n",
    "\n",
    "loss_hist, loss_val_hist, acc_val_hist = train_net(net,\n",
    "                                                   train_loader, val_loader,\n",
    "                                                   num_epochs, loss_fn, optimizer,\n",
    "                                                   accuracy_type, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff265e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
